# Assumptions
The goal is to build the system which will run the queries of specified type (filter by some attributes and count group by (other) attributes) on possibly large input data sets within 1 second.
It was assumed that loading of input data itself (people and their attributes) is not counted in performance target, as the input data may be loaded up front and only once and kept in operating memory for running the queries.

So loading of input data (in this case generation of test data) is NOT included in performance measurements (see @State classes in QueryEvaluatorPerformanceTest).

Only exactly the database operations requested were implemented (filter by all attributes active (AND) and equivalent of group by rollup). 
It is quite simple to add more operations and add more fluent interface for QueryEvaluator (something like evaluator.queryBuilder().filter().by(xxx).and().by(yyy).or.by(zzz).groupBy(aa, bb).withRollup())

# Running performance benchmarks
$sbt
> jmh:run -i 10 -wi 10 -f1 -t1 -jvmArgs="-Xmx10g"
Increasing heap size is necessary for input task sizes over 10 millions

Tests were run for both Throughput and G1 garbage collectors. To run with G1 collector:
$sbt
> jmh:run -i 10 -wi 10 -f1 -t1 -jvmArgs="-Xmx10g -XX:+UseG1GC"

Raw JMH output is included for both collectors. To see differences across collectors please run diff between JmhRawOutput.G1Collector.txt and JmhRawOutput.ParallelCollector.txt and see the very end of diff (summaries). G1 performs better, also response time is much more predictable (much smaller error).

All benchmarks were run on laptop with Intel i7-6820HQ 2.7GHz with 16GB memory, 64-bit Windows
JDK was JDK 1.8.0_112, VM 25.112-b15 (64 bit)

# How solution scales?
1. It was possible to load and process up to 50 millions records on 10GB heap. I am now constrained with max heap that I can have on my machine without memory swapping.
2. Solution scales linearly in respect to input data size (number of people)
3. Solution scales linearly in respect to number of attributes. But here the answer may not be reliable because only attributes from begining of attributes enum (low oridinals) were used in filter and group by, so the shortcut logic in BitwiseEncoder.encode may have effect. Tests here need to be rerun on other attributes.
Scaling charts included.

# Further possible optimisations
As nature of the task is massively parallel, then for larger input task sizes (over 50 millions people) and more machines available (probably cloud) I propose:
1. Split data more or less evenly across all processing nodes (split that maximum 350 million records into for example 10 subsets (35 millions each). Or achieve data locality in other way.
2. Designate master node which will listen to clients queries. 
3. Master node to send (serialize) Callable instance (which will contain QueryEvaluator.findAllAttributesMatch) into processing nodes. Each callable will process input data assigned to given processing node and will create its own GroupByRollupResult on every processing node.
The Callable instance and libraries should be placed on the processing node before serving queries starts.
_Do not send_ the actual input data to processing nodes but send pointers/indexes/offsets or similar.
3. Upon completion on all processing nodes, processing nodes to send its results to master.
4. Master to merge all instances of GroupByRollupResult (very simple map merge, also the returned and serialized back GroupByRollupResult instances are very small).
So in essence it is creation of MapReduce micro-framework, which is pointless since we have Spark and Hadoop.

Another possible optimization is to look at generated bytecode and think what can be optimised on the level of JVM instructions (the free profiler, jvisualvm does not bring any more opportunities).

Another micro-optimization is to look at GC details and try to tune particular collector. Also, which will be more efficient check if it is possible to reuse arrays in algorithm, so that they are not garbage collected.